{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Definition and Domain Alignment\n",
    "\n",
    "**Domain:** Machine Learning and Artificial Intelligence Education\n",
    "\n",
    "**Purpose:** This project builds a domain-specific chatbot that serves as an educational assistant for machine learning and AI concepts. The assistant is fine-tuned to answer questions about neural network architectures (transformers, LSTMs, CNNs), training techniques (backpropagation, gradient descent, regularization), modern AI paradigms (transfer learning, self-supervised learning, generative models), and foundational ML algorithms.\n",
    "\n",
    "**Relevance:** As ML/AI adoption accelerates across industries, there is growing demand for accessible educational tools that can explain complex concepts clearly. A domain-specific assistant fine-tuned on curated ML content provides more accurate, focused responses than a general-purpose model, reducing hallucinations and improving pedagogical value.\n",
    "\n",
    "**Approach:** Generative QA using parameter-efficient fine-tuning (QLoRA) on TinyLlama-1.1B, chosen for its balance between capability and hardware constraints (4GB VRAM GPU)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:18.516420Z",
     "start_time": "2026-02-18T23:27:18.502231Z"
    }
   },
   "source": "import torch\nimport json\nimport time\nimport gc\nimport logging\nimport sys\nimport os\nimport numpy as np\nimport pandas as pd\nfrom datasets import load_dataset, Dataset, concatenate_datasets\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    TrainingArguments,\n)\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\nfrom trl import SFTTrainer\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport evaluate\nimport nltk\nnltk.download(\"punkt_tab\", quiet=True)\n\nLOG_DIR = \"./logs\"\nos.makedirs(LOG_DIR, exist_ok=True)\n\nlogging.basicConfig(\n    level=logging.INFO,\n    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n    handlers=[\n        logging.FileHandler(f\"{LOG_DIR}/pipeline.log\", mode=\"w\"),\n        logging.StreamHandler(sys.stdout),\n    ],\n)\nlog = logging.getLogger(\"lora\")\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nlog.info(f\"device: {device}\")\nif device == \"cuda\":\n    log.info(f\"gpu: {torch.cuda.get_device_name(0)}\")\n    log.info(f\"vram: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} gb\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:18,509 [INFO] device: cuda\n",
      "2026-02-19 00:27:18,510 [INFO] gpu: NVIDIA GeForce GTX 1650 Ti\n",
      "2026-02-19 00:27:18,512 [INFO] vram: 4.0 gb\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Collection and Preprocessing\n",
    "\n",
    "**Data Sources:**\n",
    "- **databricks/databricks-dolly-15k** (Hugging Face) - 15k human-generated instruction-response pairs across multiple categories\n",
    "- **tatsu-lab/alpaca** (Hugging Face) - 52k instruction-following examples generated from text-davinci-003\n",
    "\n",
    "**Filtering Strategy:** Both datasets are filtered using keyword matching against 80+ ML/AI domain terms covering architectures, algorithms, frameworks, training concepts, and evaluation metrics. This produces a focused, domain-aligned subset.\n",
    "\n",
    "**Preprocessing Pipeline:**\n",
    "1. Load raw datasets from Hugging Face\n",
    "2. Filter for ML/AI domain relevance using keyword matching\n",
    "3. Standardize column names across sources\n",
    "4. Data cleaning: remove empty/null responses, strip whitespace, deduplicate by instruction text\n",
    "5. Combine and shuffle with fixed seed for reproducibility\n",
    "6. Format into TinyLlama chat template (system/user/assistant roles)\n",
    "7. Tokenize using LlamaTokenizer and analyze sequence lengths\n",
    "8. Split into train (90%) and eval (10%) sets"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:26.020115Z",
     "start_time": "2026-02-18T23:27:21.370226Z"
    }
   },
   "source": "dolly = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\nlog.info(f\"dolly full size: {len(dolly)}\")\nlog.info(f\"columns: {dolly.column_names}\")\nlog.info(f\"categories: {set(dolly['category'])}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:25,459 [INFO] dolly full size: 15011\n",
      "2026-02-19 00:27:25,473 [INFO] columns: ['instruction', 'context', 'response', 'category']\n",
      "2026-02-19 00:27:26,015 [INFO] categories: {'open_qa', 'brainstorming', 'general_qa', 'creative_writing', 'classification', 'summarization', 'closed_qa', 'information_extraction'}\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:33.482838Z",
     "start_time": "2026-02-18T23:27:33.468747Z"
    }
   },
   "source": "ml_keywords = [\n    \"machine learning\", \"deep learning\", \"neural network\", \"artificial intelligence\",\n    \"transformer\", \"attention mechanism\", \"lstm\", \"recurrent neural\", \"convolutional\",\n    \"backpropagation\", \"gradient descent\", \"optimizer\", \"loss function\", \"activation function\",\n    \"overfitting\", \"underfitting\", \"regularization\", \"dropout\", \"batch normalization\",\n    \"supervised learning\", \"unsupervised learning\", \"reinforcement learning\", \"transfer learning\",\n    \"natural language processing\", \"nlp\", \"computer vision\", \"generative adversarial\",\n    \"autoencoder\", \"embedding\", \"tokenization\", \"fine-tuning\", \"pre-training\",\n    \"classification\", \"regression\", \"clustering\", \"random forest\", \"decision tree\",\n    \"support vector\", \"logistic regression\", \"linear regression\", \"naive bayes\",\n    \"k-nearest\", \"cross-validation\", \"feature engineering\", \"dimensionality reduction\",\n    \"principal component\", \"pca\", \"data augmentation\", \"hyperparameter\",\n    \"bert\", \"gpt\", \"llm\", \"large language model\", \"diffusion model\",\n    \"gan\", \"vae\", \"variational\", \"self-supervised\", \"contrastive learning\",\n    \"object detection\", \"image segmentation\", \"speech recognition\", \"text generation\",\n    \"sentiment analysis\", \"named entity\", \"word2vec\", \"glove\", \"fasttext\",\n    \"tensorflow\", \"pytorch\", \"keras\", \"scikit-learn\", \"hugging face\",\n    \"epoch\", \"batch size\", \"learning rate\", \"weight decay\", \"momentum\",\n    \"softmax\", \"relu\", \"sigmoid\", \"tanh\", \"pooling layer\",\n    \"residual network\", \"resnet\", \"inception\", \"yolo\", \"u-net\",\n    \"sequence to sequence\", \"seq2seq\", \"beam search\", \"greedy decoding\",\n    \"perceptron\", \"feedforward\", \"recurrent\", \"bidirectional\",\n    \"data pipeline\", \"model deployment\", \"inference\", \"training loop\",\n    \"confusion matrix\", \"precision\", \"recall\", \"f1 score\", \"accuracy\",\n    \"roc curve\", \"auc\", \"mean squared error\", \"cross entropy\"\n]\n\ndef is_ml_related(example):\n    text = f\"{example['instruction']} {example['context']} {example['response']}\".lower()\n    return any(kw in text for kw in ml_keywords)\n\nml_dolly = dolly.filter(is_ml_related)\nlog.info(f\"ml-related entries from dolly: {len(ml_dolly)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:33,480 [INFO] ml-related entries from dolly: 2115\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:39.183878Z",
     "start_time": "2026-02-18T23:27:36.024019Z"
    }
   },
   "source": "alpaca = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\nlog.info(f\"alpaca full size: {len(alpaca)}\")\n\ndef is_ml_related_alpaca(example):\n    text = f\"{example['instruction']} {example['input']} {example['output']}\".lower()\n    return any(kw in text for kw in ml_keywords)\n\nml_alpaca = alpaca.filter(is_ml_related_alpaca)\nlog.info(f\"ml-related entries from alpaca: {len(ml_alpaca)}\")\n\nml_alpaca = ml_alpaca.rename_column(\"input\", \"context\")\nml_alpaca = ml_alpaca.rename_column(\"output\", \"response\")\nif \"text\" in ml_alpaca.column_names:\n    ml_alpaca = ml_alpaca.remove_columns([\"text\"])",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:39,150 [INFO] alpaca full size: 52002\n",
      "2026-02-19 00:27:39,172 [INFO] ml-related entries from alpaca: 5899\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:42.963324Z",
     "start_time": "2026-02-18T23:27:42.918844Z"
    }
   },
   "source": "if \"category\" not in ml_alpaca.column_names:\n    ml_alpaca = ml_alpaca.add_column(\"category\", [\"ml_education\"] * len(ml_alpaca))\n\nshared_cols = [\"instruction\", \"context\", \"response\", \"category\"]\nml_dolly_clean = ml_dolly.select_columns(shared_cols)\nml_alpaca_clean = ml_alpaca.select_columns(shared_cols)\n\ncombined = concatenate_datasets([ml_dolly_clean, ml_alpaca_clean])\nlog.info(f\"combined before cleaning: {len(combined)}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:42,960 [INFO] combined before cleaning: 8014\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "We apply several cleaning steps to ensure data quality:\n",
    "- Remove entries with empty or null instruction/response fields\n",
    "- Strip leading/trailing whitespace from all text fields\n",
    "- Remove duplicate entries based on instruction text\n",
    "- Filter out extremely short responses (less than 10 characters) that lack educational value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:46.863605Z",
     "start_time": "2026-02-18T23:27:45.393845Z"
    }
   },
   "source": "def clean_example(example):\n    example[\"instruction\"] = example[\"instruction\"].strip()\n    example[\"response\"] = example[\"response\"].strip()\n    example[\"context\"] = example[\"context\"].strip() if example[\"context\"] else \"\"\n    return example\n\ncombined = combined.map(clean_example)\n\nbefore_nulls = len(combined)\ncombined = combined.filter(lambda x: len(x[\"instruction\"]) > 0 and len(x[\"response\"]) > 0)\nlog.info(f\"removed {before_nulls - len(combined)} entries with empty instruction/response\")\n\nbefore_short = len(combined)\ncombined = combined.filter(lambda x: len(x[\"response\"]) >= 10)\nlog.info(f\"removed {before_short - len(combined)} entries with responses shorter than 10 chars\")\n\nseen_instructions = set()\nunique_indices = []\nfor i, example in enumerate(combined):\n    normalized = example[\"instruction\"].lower().strip()\n    if normalized not in seen_instructions:\n        seen_instructions.add(normalized)\n        unique_indices.append(i)\n\nbefore_dedup = len(combined)\ncombined = combined.select(unique_indices)\nlog.info(f\"removed {before_dedup - len(combined)} duplicate entries\")\n\ncombined = combined.shuffle(seed=42)\nlog.info(f\"final cleaned dataset: {len(combined)} entries\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:45,413 [INFO] removed 1 entries with empty instruction/response\n",
      "2026-02-19 00:27:45,421 [INFO] removed 76 entries with responses shorter than 10 chars\n",
      "2026-02-19 00:27:46,853 [INFO] removed 20 duplicate entries\n",
      "2026-02-19 00:27:46,859 [INFO] final cleaned dataset: 7917 entries\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:48.889300Z",
     "start_time": "2026-02-18T23:27:48.869605Z"
    }
   },
   "source": "MAX_SAMPLES = 1500\nif len(combined) > MAX_SAMPLES:\n    combined = combined.select(range(MAX_SAMPLES))\n    log.info(f\"trimmed to {MAX_SAMPLES} samples\")\n\nlog.info(f\"final dataset size: {len(combined)}\")\nlog.info(f\"sample instruction: {combined[0]['instruction'][:200]}\")\nlog.info(f\"sample response: {combined[0]['response'][:200]}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:48,878 [INFO] trimmed to 1500 samples\n",
      "2026-02-19 00:27:48,880 [INFO] final dataset size: 1500\n",
      "2026-02-19 00:27:48,883 [INFO] sample instruction: Explain how using a journal can help someone stay organized.\n",
      "2026-02-19 00:27:48,885 [INFO] sample response: Using a journal can help someone stay organized by providing a clear and consistent system for tracking tasks, notes, and thoughts. Journals can help an individual prioritize tasks, create a timeline \n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and Formatting\n",
    "\n",
    "Each example is formatted into TinyLlama's chat template with three roles:\n",
    "- **system**: sets the assistant's domain expertise\n",
    "- **user**: contains the instruction and optional context\n",
    "- **assistant**: contains the target response\n",
    "\n",
    "Tokenization is performed using the LlamaTokenizer (SentencePiece-based), which handles subword segmentation. We analyze token length distribution to set an appropriate max sequence length."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:53.017548Z",
     "start_time": "2026-02-18T23:27:51.757116Z"
    }
   },
   "source": "MODEL_NAME = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nlog.info(f\"tokenizer: {type(tokenizer).__name__}\")\nlog.info(f\"vocab size: {tokenizer.vocab_size}\")\nlog.info(f\"model max length: {tokenizer.model_max_length}\")\n\ndef format_prompt(example):\n    ctx = f\"\\nContext: {example['context']}\" if example[\"context\"].strip() else \"\"\n    text = (\n        f\"<|system|>\\n\"\n        f\"you are an expert ai/ml assistant specializing in machine learning, \"\n        f\"deep learning, neural networks, and modern ai architectures. \"\n        f\"provide clear, accurate, and educational responses.</s>\\n\"\n        f\"<|user|>\\n\"\n        f\"{example['instruction']}{ctx}</s>\\n\"\n        f\"<|assistant|>\\n\"\n        f\"{example['response']}</s>\"\n    )\n    return {\"text\": text}\n\nformatted = combined.map(format_prompt)\nlog.info(f\"formatted sample:\\n{formatted[0]['text'][:500]}\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:53,000 [INFO] tokenizer: LlamaTokenizerFast\n",
      "2026-02-19 00:27:53,001 [INFO] vocab size: 32000\n",
      "2026-02-19 00:27:53,003 [INFO] model max length: 2048\n",
      "2026-02-19 00:27:53,013 [INFO] formatted sample:\n",
      "<|system|>\n",
      "you are an expert ai/ml assistant specializing in machine learning, deep learning, neural networks, and modern ai architectures. provide clear, accurate, and educational responses.</s>\n",
      "<|user|>\n",
      "Explain how using a journal can help someone stay organized.</s>\n",
      "<|assistant|>\n",
      "Using a journal can help someone stay organized by providing a clear and consistent system for tracking tasks, notes, and thoughts. Journals can help an individual prioritize tasks, create a timeline for achieving go\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:27:58.218159Z",
     "start_time": "2026-02-18T23:27:55.497957Z"
    }
   },
   "source": "token_lengths = []\nfor example in formatted:\n    tokens = tokenizer(example[\"text\"], truncation=False)\n    token_lengths.append(len(tokens[\"input_ids\"]))\n\ntoken_lengths = np.array(token_lengths)\nlog.info(f\"token length distribution:\")\nlog.info(f\"  min: {token_lengths.min()}\")\nlog.info(f\"  mean: {token_lengths.mean():.0f}\")\nlog.info(f\"  median: {np.median(token_lengths):.0f}\")\nlog.info(f\"  max: {token_lengths.max()}\")\nlog.info(f\"  std: {token_lengths.std():.0f}\")\nlog.info(f\"  95th percentile: {np.percentile(token_lengths, 95):.0f}\")\n\nMAX_LENGTH = 256\nlog.info(f\"selected max_length: {MAX_LENGTH}\")\n\nlong_examples = (token_lengths > MAX_LENGTH).sum()\nlog.info(f\"examples that will be truncated: {long_examples} ({long_examples/len(token_lengths)*100:.1f}%)\")",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2150 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:27:58,196 [INFO] token length distribution:\n",
      "2026-02-19 00:27:58,198 [INFO]   min: 77\n",
      "2026-02-19 00:27:58,200 [INFO]   mean: 281\n",
      "2026-02-19 00:27:58,203 [INFO]   median: 184\n",
      "2026-02-19 00:27:58,205 [INFO]   max: 6671\n",
      "2026-02-19 00:27:58,207 [INFO]   std: 365\n",
      "2026-02-19 00:27:58,210 [INFO]   95th percentile: 781\n",
      "2026-02-19 00:27:58,212 [INFO] selected max_length: 256\n",
      "2026-02-19 00:27:58,213 [INFO] examples that will be truncated: 396 (26.4%)\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:28:01.098548Z",
     "start_time": "2026-02-18T23:28:01.085211Z"
    }
   },
   "source": "split = formatted.train_test_split(test_size=0.1, seed=42)\ntrain_data = split[\"train\"]\neval_data = split[\"test\"]\nlog.info(f\"train set: {len(train_data)} examples\")\nlog.info(f\"eval set: {len(eval_data)} examples\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:28:01,093 [INFO] train set: 1350 examples\n",
      "2026-02-19 00:28:01,095 [INFO] eval set: 150 examples\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading with QLoRA (4-bit Quantization)\n",
    "\n",
    "**Base Model:** TinyLlama/TinyLlama-1.1B-Chat-v1.0 from Hugging Face\n",
    "- 1.1 billion parameters\n",
    "- Pre-trained on 3 trillion tokens from SlimPajama and StarCoder\n",
    "- Chat-tuned variant with instruction-following capability\n",
    "\n",
    "**Quantization:** 4-bit NF4 (Normal Float 4) via bitsandbytes\n",
    "- Reduces model memory from ~4.4GB (FP32) to ~0.7GB\n",
    "- Double quantization further compresses quantization constants\n",
    "- Compute performed in FP16 for numerical stability\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- Targets the attention projection matrices (q, k, v, o) where most task-specific adaptation occurs\n",
    "- Rank and alpha are tuned across experiments to find the optimal balance between capacity and efficiency"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:28:04.243196Z",
     "start_time": "2026-02-18T23:28:04.231734Z"
    }
   },
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-18T23:28:11.364225Z",
     "start_time": "2026-02-18T23:28:11.330229Z"
    }
   },
   "source": "from transformers import TrainerCallback\nfrom trl import SFTConfig\n\nclass ProgressCallback(TrainerCallback):\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if logs and state.global_step > 0:\n            total = state.max_steps\n            pct = state.global_step / total * 100 if total else 0\n            parts = [f\"step {state.global_step}/{total} ({pct:.0f}%)\"]\n            if \"loss\" in logs:\n                parts.append(f\"loss={logs['loss']:.4f}\")\n            if \"eval_loss\" in logs:\n                parts.append(f\"eval_loss={logs['eval_loss']:.4f}\")\n            if \"learning_rate\" in logs:\n                parts.append(f\"lr={logs['learning_rate']:.2e}\")\n            log.info(\"  \" + \" | \".join(parts))\n\n    def on_epoch_begin(self, args, state, control, **kwargs):\n        log.info(f\"  epoch {int(state.epoch) + 1}/{int(args.num_train_epochs)} starting\")\n\n    def on_evaluate(self, args, state, control, **kwargs):\n        log.info(f\"  running evaluation at step {state.global_step}...\")\n\ndef load_base_model():\n    log.info(\"loading base model into gpu (4-bit quantization)...\")\n    mdl = AutoModelForCausalLM.from_pretrained(\n        MODEL_NAME,\n        quantization_config=bnb_config,\n        device_map=\"auto\",\n        torch_dtype=torch.float16,\n    )\n    log.info(f\"model loaded, vram: {torch.cuda.memory_allocated() / 1024**3:.2f} gb\")\n    return mdl\n\ndef cleanup_model(mdl):\n    del mdl\n    gc.collect()\n    torch.cuda.empty_cache()\n    log.info(f\"model cleaned up, vram freed: {torch.cuda.memory_allocated() / 1024**3:.2f} gb remaining\")\n\ndef run_experiment(train_data, eval_data, lora_r, lora_alpha, lr, batch_size, grad_accum, epochs, run_name):\n    exp_log_path = f\"{LOG_DIR}/{run_name}.log\"\n    exp_handler = logging.FileHandler(exp_log_path, mode=\"w\")\n    exp_handler.setFormatter(logging.Formatter(\"%(asctime)s [%(levelname)s] %(message)s\"))\n    log.addHandler(exp_handler)\n\n    log.info(f\"{'='*60}\")\n    log.info(f\"starting experiment: {run_name}\")\n    log.info(f\"  lora_r={lora_r}, lora_alpha={lora_alpha}, lr={lr}, batch={batch_size}, accum={grad_accum}, epochs={epochs}\")\n\n    mdl = load_base_model()\n\n    log.info(\"preparing model for kbit training...\")\n    mdl = prepare_model_for_kbit_training(mdl, use_gradient_checkpointing=False)\n    log.info(\"kbit preparation done\")\n\n    log.info(\"applying lora adapters...\")\n    lora_cfg = LoraConfig(\n        r=lora_r,\n        lora_alpha=lora_alpha,\n        lora_dropout=0.05,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n    )\n    mdl = get_peft_model(mdl, lora_cfg)\n\n    bf16_count = 0\n    for param in mdl.parameters():\n        if param.requires_grad and param.dtype == torch.bfloat16:\n            param.data = param.data.to(torch.float32)\n            bf16_count += 1\n    if bf16_count > 0:\n        log.info(f\"  cast {bf16_count} bf16 params to float32 (turing gpu compatibility)\")\n\n    trainable, total = 0, 0\n    for p in mdl.parameters():\n        total += p.numel()\n        if p.requires_grad:\n            trainable += p.numel()\n    log.info(f\"  lora applied: {trainable:,} trainable / {total:,} total ({100 * trainable / total:.2f}%)\")\n\n    output_dir = f\"./lora-ml-assistant/{run_name}\"\n\n    args = SFTConfig(\n        output_dir=output_dir,\n        num_train_epochs=epochs,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size,\n        gradient_accumulation_steps=grad_accum,\n        gradient_checkpointing=False,\n        eval_strategy=\"steps\",\n        eval_steps=50,\n        save_strategy=\"steps\",\n        save_steps=50,\n        logging_steps=10,\n        learning_rate=lr,\n        weight_decay=0.01,\n        warmup_ratio=0.1,\n        lr_scheduler_type=\"cosine\",\n        fp16=False,\n        bf16=False,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_loss\",\n        report_to=\"none\",\n        optim=\"adamw_torch\",\n        max_length=MAX_LENGTH,\n        dataset_text_field=\"text\",\n    )\n\n    log.info(\"initializing trainer (tokenizing dataset, this may take a few minutes)...\")\n    trainer = SFTTrainer(\n        model=mdl,\n        train_dataset=train_data,\n        eval_dataset=eval_data,\n        processing_class=tokenizer,\n        args=args,\n    )\n    trainer.add_callback(ProgressCallback())\n    log.info(f\"trainer ready. total steps: {trainer.state.max_steps if hasattr(trainer.state, 'max_steps') else 'calculating...'}\")\n\n    torch.cuda.reset_peak_memory_stats()\n    start = time.time()\n    log.info(\"training started\")\n    result = trainer.train()\n    elapsed = time.time() - start\n    peak_mem = torch.cuda.max_memory_allocated() / 1024**3\n\n    logs = trainer.state.log_history\n    eval_losses = [l[\"eval_loss\"] for l in logs if \"eval_loss\" in l]\n    best_eval = min(eval_losses) if eval_losses else None\n\n    log.info(f\"saving model to {output_dir}/final ...\")\n    trainer.save_model(f\"{output_dir}/final\")\n    tokenizer.save_pretrained(f\"{output_dir}/final\")\n\n    log.info(f\"  train loss: {result.training_loss:.4f}\")\n    log.info(f\"  best eval loss: {best_eval:.4f}\" if best_eval else \"  no eval loss recorded\")\n    log.info(f\"  peak vram: {peak_mem:.2f} gb\")\n    log.info(f\"  time: {elapsed/60:.1f} min\")\n    log.info(f\"experiment {run_name} complete\")\n    log.info(f\"{'='*60}\")\n\n    with open(f\"{LOG_DIR}/{run_name}_history.json\", \"w\") as f:\n        json.dump(logs, f, indent=2)\n\n    log.removeHandler(exp_handler)\n    exp_handler.close()\n\n    cleanup_model(mdl)\n\n    return {\n        \"experiment\": run_name,\n        \"learning_rate\": str(lr),\n        \"batch_size\": f\"{batch_size} (accum {grad_accum})\",\n        \"effective_batch\": batch_size * grad_accum,\n        \"epochs\": epochs,\n        \"lora_r\": lora_r,\n        \"lora_alpha\": lora_alpha,\n        \"trainable_params\": f\"{trainable:,}\",\n        \"train_loss\": round(result.training_loss, 4),\n        \"best_eval_loss\": round(best_eval, 4) if best_eval else None,\n        \"peak_vram_gb\": round(peak_mem, 2),\n        \"training_time_min\": round(elapsed / 60, 1),\n        \"save_path\": f\"{output_dir}/final\",\n    }",
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Hyperparameter Experiments\n\nWe run three experiments varying key hyperparameters to find the optimal configuration. Each experiment tracks training loss, evaluation loss, GPU memory usage, and training time.\n\n| Experiment | Variable Changed | Rationale |\n|---|---|---|\n| Run 1 (Baseline) | Default config | Establishes baseline performance with moderate settings |\n| Run 2 | Lower learning rate, higher LoRA rank | Tests whether more capacity with slower learning improves quality |\n| Run 3 | Larger effective batch, fewer epochs | Tests whether larger batches with less training provides better generalization |\n\nNote: Training uses float32 precision (fp16 disabled due to bitsandbytes/Turing GPU bf16 incompatibility). The 4-bit quantization still provides significant memory savings."
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T03:18:07.875849Z",
     "start_time": "2026-02-18T23:28:14.241212Z"
    }
   },
   "source": "experiment_results = []\n\nresult_1 = run_experiment(\n    train_data, eval_data,\n    lora_r=16, lora_alpha=32,\n    lr=2e-4, batch_size=2, grad_accum=8, epochs=2,\n    run_name=\"run1_baseline\"\n)\nexperiment_results.append(result_1)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:28:14,244 [INFO] ============================================================\n",
      "2026-02-19 00:28:14,246 [INFO] starting experiment: run1_baseline\n",
      "2026-02-19 00:28:14,247 [INFO]   lora_r=16, lora_alpha=32, lr=0.0002, batch=2, accum=8, epochs=2\n",
      "2026-02-19 00:28:14,249 [INFO] loading base model into gpu (4-bit quantization)...\n",
      "2026-02-19 00:28:14,921 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-02-19 00:28:22,793 [INFO] model loaded, vram: 2.91 gb\n",
      "2026-02-19 00:28:22,793 [INFO] preparing model for kbit training...\n",
      "2026-02-19 00:28:22,814 [INFO] kbit preparation done\n",
      "2026-02-19 00:28:22,815 [INFO] applying lora adapters...\n",
      "2026-02-19 00:28:23,106 [INFO]   lora applied: 4,505,600 trainable / 620,111,872 total (0.73%)\n",
      "2026-02-19 00:28:23,171 [INFO] initializing trainer (tokenizing dataset, this may take a few minutes)...\n",
      "2026-02-19 00:28:23,378 [INFO] trainer ready. total steps: 0\n",
      "2026-02-19 00:28:23,379 [INFO] training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:28:23,917 [INFO]   epoch 1/2 starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 3:48:24, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.098300</td>\n",
       "      <td>1.090436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.072600</td>\n",
       "      <td>1.073280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.142000</td>\n",
       "      <td>1.069398</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 00:41:41,633 [INFO]   step 10/170 (6%) | loss=1.9526 | lr=1.06e-04\n",
      "2026-02-19 00:54:32,889 [INFO]   step 20/170 (12%) | loss=1.5472 | lr=2.00e-04\n",
      "2026-02-19 01:07:20,989 [INFO]   step 30/170 (18%) | loss=1.2246 | lr=1.97e-04\n",
      "2026-02-19 01:19:34,846 [INFO]   step 40/170 (24%) | loss=1.1375 | lr=1.90e-04\n",
      "2026-02-19 01:32:27,046 [INFO]   step 50/170 (29%) | loss=1.0983 | lr=1.79e-04\n",
      "2026-02-19 01:36:43,653 [INFO]   step 50/170 (29%) | eval_loss=1.0904\n",
      "2026-02-19 01:36:43,659 [INFO]   running evaluation at step 50...\n",
      "2026-02-19 01:49:51,676 [INFO]   step 60/170 (35%) | loss=1.1131 | lr=1.65e-04\n",
      "2026-02-19 02:02:38,611 [INFO]   step 70/170 (41%) | loss=1.1083 | lr=1.48e-04\n",
      "2026-02-19 02:15:00,037 [INFO]   step 80/170 (47%) | loss=1.0822 | lr=1.29e-04\n",
      "2026-02-19 02:20:39,022 [INFO]   epoch 2/2 starting\n",
      "2026-02-19 02:26:54,429 [INFO]   step 90/170 (53%) | loss=1.0840 | lr=1.09e-04\n",
      "2026-02-19 02:39:58,690 [INFO]   step 100/170 (59%) | loss=1.0726 | lr=8.87e-05\n",
      "2026-02-19 02:44:17,840 [INFO]   step 100/170 (59%) | eval_loss=1.0733\n",
      "2026-02-19 02:44:17,845 [INFO]   running evaluation at step 100...\n",
      "2026-02-19 02:57:07,353 [INFO]   step 110/170 (65%) | loss=1.0677 | lr=6.87e-05\n",
      "2026-02-19 03:09:57,086 [INFO]   step 120/170 (71%) | loss=1.1117 | lr=5.00e-05\n",
      "2026-02-19 03:22:28,598 [INFO]   step 130/170 (76%) | loss=1.1163 | lr=3.34e-05\n",
      "2026-02-19 03:35:00,435 [INFO]   step 140/170 (82%) | loss=1.1078 | lr=1.96e-05\n",
      "2026-02-19 03:47:56,685 [INFO]   step 150/170 (88%) | loss=1.1420 | lr=9.15e-06\n",
      "2026-02-19 03:52:14,907 [INFO]   step 150/170 (88%) | eval_loss=1.0694\n",
      "2026-02-19 03:52:14,912 [INFO]   running evaluation at step 150...\n",
      "2026-02-19 04:05:09,772 [INFO]   step 160/170 (94%) | loss=1.0660 | lr=2.54e-06\n",
      "2026-02-19 04:18:01,297 [INFO]   step 170/170 (100%) | loss=1.1233 | lr=2.11e-08\n",
      "2026-02-19 04:18:03,606 [INFO]   step 170/170 (100%)\n",
      "2026-02-19 04:18:03,620 [INFO] saving model to ./lora-ml-assistant/run1_baseline/final ...\n",
      "2026-02-19 04:18:04,690 [INFO]   train loss: 1.1856\n",
      "2026-02-19 04:18:04,692 [INFO]   best eval loss: 1.0694\n",
      "2026-02-19 04:18:04,693 [INFO]   peak vram: 4.53 gb\n",
      "2026-02-19 04:18:04,695 [INFO]   time: 229.7 min\n",
      "2026-02-19 04:18:04,697 [INFO] experiment run1_baseline complete\n",
      "2026-02-19 04:18:04,699 [INFO] ============================================================\n",
      "2026-02-19 04:18:07,863 [INFO] model cleaned up, vram freed: 2.59 gb remaining\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T07:41:33.765868Z",
     "start_time": "2026-02-19T03:44:33.405913Z"
    }
   },
   "source": "result_2 = run_experiment(\n    train_data, eval_data,\n    lora_r=32, lora_alpha=64,\n    lr=5e-5, batch_size=2, grad_accum=8, epochs=2,\n    run_name=\"run2_higher_rank_lower_lr\"\n)\nexperiment_results.append(result_2)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 04:44:33,411 [INFO] ============================================================\n",
      "2026-02-19 04:44:33,413 [INFO] starting experiment: run2_higher_rank_lower_lr\n",
      "2026-02-19 04:44:33,414 [INFO]   lora_r=32, lora_alpha=64, lr=5e-05, batch=2, accum=8, epochs=2\n",
      "2026-02-19 04:44:33,416 [INFO] loading base model into gpu (4-bit quantization)...\n",
      "2026-02-19 04:44:34,392 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-02-19 04:44:49,576 [INFO] model loaded, vram: 2.78 gb\n",
      "2026-02-19 04:44:49,577 [INFO] preparing model for kbit training...\n",
      "2026-02-19 04:44:49,597 [INFO] kbit preparation done\n",
      "2026-02-19 04:44:49,599 [INFO] applying lora adapters...\n",
      "2026-02-19 04:44:50,340 [INFO]   lora applied: 9,011,200 trainable / 624,617,472 total (1.44%)\n",
      "2026-02-19 04:44:50,449 [INFO] initializing trainer (tokenizing dataset, this may take a few minutes)...\n",
      "2026-02-19 04:44:50,992 [INFO] trainer ready. total steps: 0\n",
      "2026-02-19 04:44:50,995 [INFO] training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 04:44:51,480 [INFO]   epoch 1/2 starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='170' max='170' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [170/170 3:55:17, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.150200</td>\n",
       "      <td>1.134109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.105300</td>\n",
       "      <td>1.092605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.172200</td>\n",
       "      <td>1.089462</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 04:58:49,102 [INFO]   step 10/170 (6%) | loss=1.9855 | lr=2.65e-05\n",
      "2026-02-19 05:12:32,743 [INFO]   step 20/170 (12%) | loss=1.7993 | lr=5.00e-05\n",
      "2026-02-19 05:25:59,506 [INFO]   step 30/170 (18%) | loss=1.4687 | lr=4.92e-05\n",
      "2026-02-19 05:39:01,140 [INFO]   step 40/170 (24%) | loss=1.2202 | lr=4.75e-05\n",
      "2026-02-19 05:51:56,154 [INFO]   step 50/170 (29%) | loss=1.1502 | lr=4.48e-05\n",
      "2026-02-19 05:56:31,308 [INFO]   step 50/170 (29%) | eval_loss=1.1341\n",
      "2026-02-19 05:56:31,312 [INFO]   running evaluation at step 50...\n",
      "2026-02-19 06:09:51,901 [INFO]   step 60/170 (35%) | loss=1.1474 | lr=4.13e-05\n",
      "2026-02-19 06:22:58,713 [INFO]   step 70/170 (41%) | loss=1.1322 | lr=3.71e-05\n",
      "2026-02-19 06:35:40,825 [INFO]   step 80/170 (47%) | loss=1.1038 | lr=3.23e-05\n",
      "2026-02-19 06:41:29,627 [INFO]   epoch 2/2 starting\n",
      "2026-02-19 06:48:03,456 [INFO]   step 90/170 (53%) | loss=1.1113 | lr=2.73e-05\n",
      "2026-02-19 07:01:18,475 [INFO]   step 100/170 (59%) | loss=1.1053 | lr=2.22e-05\n",
      "2026-02-19 07:05:51,815 [INFO]   step 100/170 (59%) | eval_loss=1.0926\n",
      "2026-02-19 07:05:51,819 [INFO]   running evaluation at step 100...\n",
      "2026-02-19 07:18:55,829 [INFO]   step 110/170 (65%) | loss=1.0984 | lr=1.72e-05\n",
      "2026-02-19 07:31:59,830 [INFO]   step 120/170 (71%) | loss=1.1434 | lr=1.25e-05\n",
      "2026-02-19 07:44:49,950 [INFO]   step 130/170 (76%) | loss=1.1460 | lr=8.35e-06\n",
      "2026-02-19 07:57:37,930 [INFO]   step 140/170 (82%) | loss=1.1397 | lr=4.90e-06\n",
      "2026-02-19 08:11:00,149 [INFO]   step 150/170 (88%) | loss=1.1722 | lr=2.29e-06\n",
      "2026-02-19 08:15:34,597 [INFO]   step 150/170 (88%) | eval_loss=1.0895\n",
      "2026-02-19 08:15:34,601 [INFO]   running evaluation at step 150...\n",
      "2026-02-19 08:28:43,276 [INFO]   step 160/170 (94%) | loss=1.0951 | lr=6.35e-07\n",
      "2026-02-19 08:41:28,466 [INFO]   step 170/170 (100%) | loss=1.1537 | lr=5.27e-09\n",
      "2026-02-19 08:41:31,692 [INFO]   step 170/170 (100%)\n",
      "2026-02-19 08:41:31,700 [INFO] saving model to ./lora-ml-assistant/run2_higher_rank_lower_lr/final ...\n",
      "2026-02-19 08:41:33,099 [INFO]   train loss: 1.2454\n",
      "2026-02-19 08:41:33,101 [INFO]   best eval loss: 1.0895\n",
      "2026-02-19 08:41:33,102 [INFO]   peak vram: 4.56 gb\n",
      "2026-02-19 08:41:33,103 [INFO]   time: 236.7 min\n",
      "2026-02-19 08:41:33,105 [INFO] experiment run2_higher_rank_lower_lr complete\n",
      "2026-02-19 08:41:33,106 [INFO] ============================================================\n",
      "2026-02-19 08:41:33,753 [INFO] model cleaned up, vram freed: 2.62 gb remaining\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:03:24.776495Z",
     "start_time": "2026-02-19T07:47:43.266327Z"
    }
   },
   "source": "result_3 = run_experiment(\n    train_data, eval_data,\n    lora_r=16, lora_alpha=32,\n    lr=1e-4, batch_size=4, grad_accum=4, epochs=1,\n    run_name=\"run3_larger_batch_fewer_epochs\"\n)\nexperiment_results.append(result_3)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 08:47:43,271 [INFO] ============================================================\n",
      "2026-02-19 08:47:43,273 [INFO] starting experiment: run3_larger_batch_fewer_epochs\n",
      "2026-02-19 08:47:43,275 [INFO]   lora_r=16, lora_alpha=32, lr=0.0001, batch=4, accum=4, epochs=1\n",
      "2026-02-19 08:47:43,277 [INFO] loading base model into gpu (4-bit quantization)...\n",
      "2026-02-19 08:47:45,125 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-02-19 08:47:57,045 [INFO] model loaded, vram: 2.78 gb\n",
      "2026-02-19 08:47:57,048 [INFO] preparing model for kbit training...\n",
      "2026-02-19 08:47:57,090 [INFO] kbit preparation done\n",
      "2026-02-19 08:47:57,092 [INFO] applying lora adapters...\n",
      "2026-02-19 08:47:58,012 [INFO]   lora applied: 4,505,600 trainable / 620,111,872 total (0.73%)\n",
      "2026-02-19 08:47:58,100 [INFO] initializing trainer (tokenizing dataset, this may take a few minutes)...\n",
      "2026-02-19 08:47:58,389 [INFO] trainer ready. total steps: 0\n",
      "2026-02-19 08:47:58,390 [INFO] training started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 08:47:58,800 [INFO]   epoch 1/1 starting\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='85' max='85' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [85/85 2:13:52, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.125300</td>\n",
       "      <td>1.141566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 09:02:18,175 [INFO]   step 10/85 (12%) | loss=1.9561 | lr=1.00e-04\n",
      "2026-02-19 09:17:27,599 [INFO]   step 20/85 (24%) | loss=1.6081 | lr=9.58e-05\n",
      "2026-02-19 09:33:43,582 [INFO]   step 30/85 (35%) | loss=1.2766 | lr=8.39e-05\n",
      "2026-02-19 09:48:53,192 [INFO]   step 40/85 (47%) | loss=1.1793 | lr=6.62e-05\n",
      "2026-02-19 10:03:19,009 [INFO]   step 50/85 (59%) | loss=1.1253 | lr=4.59e-05\n",
      "2026-02-19 10:08:04,219 [INFO]   step 50/85 (59%) | eval_loss=1.1416\n",
      "2026-02-19 10:08:04,229 [INFO]   running evaluation at step 50...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 780da934-64fa-438a-964f-b51742c34923)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:15,067 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 780da934-64fa-438a-964f-b51742c34923)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:15,079 [WARNING] Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b757b37a-bb3f-4d70-a077-c704d0f7dc77)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:26,266 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b757b37a-bb3f-4d70-a077-c704d0f7dc77)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:26,269 [WARNING] Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6984b690-dff6-4462-a2c4-2d5483b28fe6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:38,382 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 6984b690-dff6-4462-a2c4-2d5483b28fe6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:38,385 [WARNING] Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: de972189-e10b-4f1c-9391-ee76d15927a0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:52,524 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: de972189-e10b-4f1c-9391-ee76d15927a0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:08:52,525 [WARNING] Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7efa3593-48f1-4290-8447-cc293a469660)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:09:10,623 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7efa3593-48f1-4290-8447-cc293a469660)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:09:10,625 [WARNING] Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f63c0a9c-a50b-496b-960b-1e5f54e94aa6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:09:28,758 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f63c0a9c-a50b-496b-960b-1e5f54e94aa6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\PycharmProjects\\LoRA\\lora-venv\\Lib\\site-packages\\peft\\utils\\other.py:1394: UserWarning: Unable to fetch remote file due to the following error (ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: f63c0a9c-a50b-496b-960b-1e5f54e94aa6)') - silently ignoring the lookup for the file config.json in TinyLlama/TinyLlama-1.1B-Chat-v1.0.\n",
      "  warnings.warn(\n",
      "C:\\Users\\user\\PycharmProjects\\LoRA\\lora-venv\\Lib\\site-packages\\peft\\utils\\save_and_load.py:295: UserWarning: Could not find a config file in TinyLlama/TinyLlama-1.1B-Chat-v1.0 - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 10:24:28,834 [INFO]   step 60/85 (71%) | loss=1.1367 | lr=2.62e-05\n",
      "2026-02-19 10:40:14,404 [INFO]   step 70/85 (82%) | loss=1.1319 | lr=1.05e-05\n",
      "2026-02-19 10:56:25,173 [INFO]   step 80/85 (94%) | loss=1.1086 | lr=1.53e-06\n",
      "2026-02-19 11:03:17,349 [INFO]   step 85/85 (100%)\n",
      "2026-02-19 11:03:17,390 [INFO] saving model to ./lora-ml-assistant/run3_larger_batch_fewer_epochs/final ...\n",
      "2026-02-19 11:03:18,794 [INFO]   train loss: 1.3080\n",
      "2026-02-19 11:03:18,798 [INFO]   best eval loss: 1.1416\n",
      "2026-02-19 11:03:18,801 [INFO]   peak vram: 6.44 gb\n",
      "2026-02-19 11:03:18,803 [INFO]   time: 135.3 min\n",
      "2026-02-19 11:03:18,805 [INFO] experiment run3_larger_batch_fewer_epochs complete\n",
      "2026-02-19 11:03:18,808 [INFO] ============================================================\n",
      "2026-02-19 11:03:24,758 [INFO] model cleaned up, vram freed: 2.59 gb remaining\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Results Table\n",
    "\n",
    "The table below documents all experiments with their hyperparameters, performance metrics, GPU memory usage, and training time."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:13:20.287862Z",
     "start_time": "2026-02-19T10:13:20.159835Z"
    }
   },
   "source": "experiments_df = pd.DataFrame(experiment_results)\ndisplay_cols = [\n    \"experiment\", \"learning_rate\", \"batch_size\", \"epochs\",\n    \"lora_r\", \"lora_alpha\", \"trainable_params\",\n    \"train_loss\", \"best_eval_loss\", \"peak_vram_gb\", \"training_time_min\"\n]\nlog.info(\"experiment tracking table:\")\nlog.info(\"\\n\" + experiments_df[display_cols].to_string(index=False))\n\nbest_run = min(experiment_results, key=lambda x: x[\"best_eval_loss\"] if x[\"best_eval_loss\"] else float(\"inf\"))\nlog.info(f\"best experiment: {best_run['experiment']} (eval loss: {best_run['best_eval_loss']})\")\n\nexperiments_df[display_cols].to_csv(f\"{LOG_DIR}/experiments.csv\", index=False)\nlog.info(f\"experiments table saved to {LOG_DIR}/experiments.csv\")\n\nBEST_MODEL_PATH = best_run[\"save_path\"]",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:13:20,196 [INFO] experiment tracking table:\n",
      "2026-02-19 11:13:20,263 [INFO] \n",
      "                    experiment learning_rate  batch_size  epochs  lora_r  lora_alpha trainable_params  train_loss  best_eval_loss  peak_vram_gb  training_time_min\n",
      "                 run1_baseline        0.0002 2 (accum 8)       2      16          32        4,505,600      1.1856          1.0694          4.53              229.7\n",
      "     run2_higher_rank_lower_lr         5e-05 2 (accum 8)       2      32          64        9,011,200      1.2454          1.0895          4.56              236.7\n",
      "run3_larger_batch_fewer_epochs        0.0001 4 (accum 4)       1      16          32        4,505,600      1.3080          1.1416          6.44              135.3\n",
      "2026-02-19 11:13:20,265 [INFO] best experiment: run1_baseline (eval loss: 1.0694)\n",
      "2026-02-19 11:13:20,284 [INFO] experiments table saved to ./logs/experiments.csv\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance Evaluation\n",
    "\n",
    "We evaluate the fine-tuned model against the base model using multiple metrics:\n",
    "\n",
    "- **BLEU Score**: Measures n-gram precision between generated and reference text\n",
    "- **ROUGE-1/2/L**: Measures recall-oriented overlap at unigram, bigram, and longest common subsequence levels\n",
    "- **F1 Score**: Token-level F1 measuring precision and recall of generated tokens against references\n",
    "- **Perplexity**: Measures model confidence (lower = better) on held-out evaluation data\n",
    "- **Qualitative Testing**: Side-by-side comparison on domain and out-of-domain queries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:14:22.822082Z",
     "start_time": "2026-02-19T10:14:22.812331Z"
    }
   },
   "source": [
    "test_questions = [\n",
    "    \"what is a transformer architecture in deep learning?\",\n",
    "    \"explain the difference between lstm and gru networks\",\n",
    "    \"what is backpropagation and why is it important?\",\n",
    "    \"how does dropout regularization work?\",\n",
    "    \"what is transfer learning and when should you use it?\",\n",
    "    \"explain the attention mechanism in neural networks\",\n",
    "    \"what is the vanishing gradient problem?\",\n",
    "    \"how do generative adversarial networks work?\",\n",
    "    \"what is the difference between supervised and unsupervised learning?\",\n",
    "    \"explain batch normalization and its benefits\",\n",
    "]\n",
    "\n",
    "reference_answers = [\n",
    "    \"a transformer is a neural network architecture that uses self-attention mechanisms to process sequential data in parallel, unlike rnns. it consists of encoder and decoder blocks with multi-head attention layers, enabling it to capture long-range dependencies efficiently. transformers are the foundation of models like bert and gpt.\",\n",
    "    \"lstm uses three gates (input, forget, output) and a cell state to control information flow, while gru simplifies this to two gates (reset, update) making it computationally faster. both address the vanishing gradient problem in rnns, but gru has fewer parameters and trains faster with comparable performance on many tasks.\",\n",
    "    \"backpropagation is an algorithm for computing gradients of the loss function with respect to network weights by applying the chain rule from output to input layers. it is essential because it enables gradient-based optimization methods to update weights and minimize the loss during training.\",\n",
    "    \"dropout randomly deactivates a fraction of neurons during training, forcing the network to learn redundant representations and reducing co-adaptation between neurons. this acts as regularization, preventing overfitting by making the model more robust and improving generalization to unseen data.\",\n",
    "    \"transfer learning involves taking a model pre-trained on a large dataset and adapting it to a new related task. it is useful when you have limited training data, as the pre-trained model already captures general features that can be fine-tuned for the specific task with fewer examples.\",\n",
    "    \"the attention mechanism allows a model to focus on different parts of the input sequence when producing each output element. it computes weighted sums of values based on compatibility scores between queries and keys, enabling the model to capture relevant relationships regardless of distance in the sequence.\",\n",
    "    \"the vanishing gradient problem occurs when gradients become extremely small as they are propagated back through many layers, effectively preventing early layers from learning. this is common in deep networks and rnns, and is addressed by architectures like lstm, residual connections, and careful initialization.\",\n",
    "    \"gans consist of two networks: a generator that creates fake data and a discriminator that distinguishes real from fake. they are trained adversarially, with the generator trying to fool the discriminator. this competition drives both networks to improve, resulting in realistic synthetic data generation.\",\n",
    "    \"supervised learning uses labeled data where each input has a corresponding target output, used for tasks like classification and regression. unsupervised learning works with unlabeled data to find hidden patterns or structure, used for clustering, dimensionality reduction, and anomaly detection.\",\n",
    "    \"batch normalization normalizes the inputs of each layer to have zero mean and unit variance across the mini-batch. this stabilizes training by reducing internal covariate shift, allows higher learning rates, acts as mild regularization, and generally leads to faster convergence.\",\n",
    "]\n",
    "\n",
    "out_of_domain_questions = [\n",
    "    \"what is the capital of france?\",\n",
    "    \"how do i bake a chocolate cake?\",\n",
    "    \"who won the 2022 world cup?\",\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 44
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:14:34.566579Z",
     "start_time": "2026-02-19T10:14:34.556568Z"
    }
   },
   "source": [
    "def generate_response(mdl, tok, question, max_new_tokens=256):\n",
    "    prompt = (\n",
    "        f\"<|system|>\\n\"\n",
    "        f\"you are an expert ai/ml assistant specializing in machine learning, \"\n",
    "        f\"deep learning, neural networks, and modern ai architectures. \"\n",
    "        f\"provide clear, accurate, and educational responses.</s>\\n\"\n",
    "        f\"<|user|>\\n{question}</s>\\n\"\n",
    "        f\"<|assistant|>\\n\"\n",
    "    )\n",
    "    inputs = tok(prompt, return_tensors=\"pt\").to(mdl.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = mdl.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tok.eos_token_id,\n",
    "        )\n",
    "    response = tok.decode(outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True)\n",
    "    return response.strip()"
   ],
   "outputs": [],
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:20:53.099067Z",
     "start_time": "2026-02-19T10:14:38.513194Z"
    }
   },
   "source": "ft_model = load_base_model()\nft_model = PeftModel.from_pretrained(ft_model, BEST_MODEL_PATH)\n\nlog.info(\"generating fine-tuned model responses on domain questions\")\nfinetuned_responses = []\nfor q in test_questions:\n    resp = generate_response(ft_model, tokenizer, q)\n    finetuned_responses.append(resp)\n    log.info(f\"q: {q}\")\n    log.info(f\"a: {resp[:300]}\\n\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:14:38,518 [INFO] loading base model into gpu (4-bit quantization)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bce431a9-a526-45a9-bc6a-d4fbe1aa284a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:14:48,675 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bce431a9-a526-45a9-bc6a-d4fbe1aa284a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:14:48,676 [WARNING] Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 48c29d9e-bc1d-49c5-8e6d-15f603abec0f)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:14:59,944 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 48c29d9e-bc1d-49c5-8e6d-15f603abec0f)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:14:59,945 [WARNING] Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c20c53f0-5437-432a-b3b5-0cec7087a493)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:12,140 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: c20c53f0-5437-432a-b3b5-0cec7087a493)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:12,145 [WARNING] Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b8863ea3-7c17-4bd1-95e6-c85cd1f7ac61)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:26,228 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b8863ea3-7c17-4bd1-95e6-c85cd1f7ac61)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:26,230 [WARNING] Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 02b095bd-21d0-4e70-be2c-25fc987f70b0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:44,307 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 02b095bd-21d0-4e70-be2c-25fc987f70b0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:15:44,308 [WARNING] Retrying in 8s [Retry 5/5].\n",
      "2026-02-19 11:15:59,436 [INFO] model loaded, vram: 2.78 gb\n",
      "2026-02-19 11:15:59,909 [INFO] generating fine-tuned model responses on domain questions\n",
      "2026-02-19 11:16:26,213 [INFO] q: what is a transformer architecture in deep learning?\n",
      "2026-02-19 11:16:26,215 [INFO] a: A Transformer Architecture (Transformer) is a state-of-the-art natural language processing model that uses stacked encoderdecoders to process text data. It consists of multiple layers with attention mechanisms to capture contextual information from the input sequence while also learning representat\n",
      "\n",
      "2026-02-19 11:17:00,888 [INFO] q: explain the difference between lstm and gru networks\n",
      "2026-02-19 11:17:00,889 [INFO] a: LSTM (Long Short-Term Memory) is a type of recurrent neural network that can process sequential data such as text or speech. It uses a hidden state to remember previous input sequences and output them with a probability distribution over time. This allows it to create complex hierarchical structures\n",
      "\n",
      "2026-02-19 11:17:22,654 [INFO] q: what is backpropagation and why is it important?\n",
      "2026-02-19 11:17:22,656 [INFO] a: Backpropagation (BP) is a type of artificial intelligence algorithm that trains a neural network to accurately identify patterns within data. It uses error signals generated by the training process as feedback, which helps the network adjust its weights over time. By doing so, BP can learn from past\n",
      "\n",
      "2026-02-19 11:17:50,342 [INFO] q: how does dropout regularization work?\n",
      "2026-02-19 11:17:50,343 [INFO] a: Dropout is a technique that can be used to regularize the output of a deep neural network by randomly removing neurons from its input layers. This allows for the model to learn without overfitting or getting stuck in local minima. By adding random noise to each input sample, Dropout prevents the mod\n",
      "\n",
      "2026-02-19 11:18:14,580 [INFO] q: what is transfer learning and when should you use it?\n",
      "2026-02-19 11:18:14,583 [INFO] a: Transfer Learning is a technique that involves taking pre-trained models from other tasks to train on new data. The benefit of using Transfer Learning is that the model can be trained with little or no additional training time compared to traditional methods for similar problems. However, this metho\n",
      "\n",
      "2026-02-19 11:18:58,852 [INFO] q: explain the attention mechanism in neural networks\n",
      "2026-02-19 11:18:58,853 [INFO] a: The attention mechanism is a type of recurrent neural network that involves attending to parts of a sequence. In this way, it allows the model to learn which aspects of a sentence or image should be given more weight than others during inference time. This can help improve accuracy by allowing for b\n",
      "\n",
      "2026-02-19 11:19:32,713 [INFO] q: what is the vanishing gradient problem?\n",
      "2026-02-19 11:19:32,714 [INFO] a: The vanishing gradient problem is a commonly occurring issue when using Deep Learning models to solve problems such as image classification or regression. In this case, it occurs because during training, the model starts by optimising its weights towards what it thinks is the best solution for each \n",
      "\n",
      "2026-02-19 11:20:03,414 [INFO] q: how do generative adversarial networks work?\n",
      "2026-02-19 11:20:03,415 [INFO] a: Generative Adversarial Networks (GANs) is a type of artificial intelligence algorithm that generates realistic images by training two separate models to compete with each other in creating artwork. The process starts with an initial model called the generator, which creates random noise samples to t\n",
      "\n",
      "2026-02-19 11:20:27,107 [INFO] q: what is the difference between supervised and unsupervised learning?\n",
      "2026-02-19 11:20:27,108 [INFO] a: Supervised Learning: Supervised learning involves providing labeled data to a model and using that as input for training. This process helps models learn from past experiences so they can better predict future outcomes. Unsupervised Learning: Unsupervised learning doesnt require any labelled data b\n",
      "\n",
      "2026-02-19 11:20:53,095 [INFO] q: explain batch normalization and its benefits\n",
      "2026-02-19 11:20:53,096 [INFO] a: Batch Normalization (BN) is a technique used to regularize the gradient of convolutional layers during training, improving their stability and accuracy by reducing the variability of activations caused by overfitting. Batch Normalization can also help prevent the vanishing or exploding gradients tha\n",
      "\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:24:44.998499Z",
     "start_time": "2026-02-19T10:23:01.331799Z"
    }
   },
   "source": "log.info(\"generating fine-tuned model responses on out-of-domain questions\")\nfor q in out_of_domain_questions:\n    resp = generate_response(ft_model, tokenizer, q)\n    log.info(f\"q: {q}\")\n    log.info(f\"a: {resp[:300]}\\n\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:23:01,334 [INFO] generating fine-tuned model responses on out-of-domain questions\n",
      "2026-02-19 11:23:32,768 [INFO] q: what is the capital of france?\n",
      "2026-02-19 11:23:32,769 [INFO] a: The capital city of France is Paris, which has been its capital since the early Middle Ages (12th century). It is located on the Ile de la Cit, a small island surrounded by the Seine River. The most famous landmark on this island is the Eiffel Tower, built during the French Revolution as part of th\n",
      "\n",
      "2026-02-19 11:24:20,231 [INFO] q: how do i bake a chocolate cake?\n",
      "2026-02-19 11:24:20,232 [INFO] a: To make a classic chocolate cake recipe:\n",
      "1. Preheat your oven to 350 degrees Fahrenheit (176C).\n",
      "2. Grease or line your muffin tin with cupcake liners. Set aside.\n",
      "3. In a medium bowl whisk together flour, sugar, salt, baking powder, and cocoa powder. Set aside.\n",
      "4. In another small bowl mix together \n",
      "\n",
      "2026-02-19 11:24:44,993 [INFO] q: who won the 2022 world cup?\n",
      "2026-02-19 11:24:44,994 [INFO] a: The 2022 FIFA World Cup was held in Qatar from 21 November to 18 December 2022. The tournament featured teams representing each of the 32 UEFA (Union of European Football Associations) member countries and two confederation champions as well as hosts Qatar. Argentina emerged victorious by defeating \n",
      "\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:36:19.005369Z",
     "start_time": "2026-02-19T10:24:59.394654Z"
    }
   },
   "source": "cleanup_model(ft_model)\n\nbase_model = load_base_model()\n\nlog.info(\"generating base model responses on domain questions\")\nbase_responses = []\nfor q in test_questions:\n    resp = generate_response(base_model, tokenizer, q)\n    base_responses.append(resp)\n    log.info(f\"q: {q}\")\n    log.info(f\"a: {resp[:300]}\\n\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:24:59,799 [INFO] model cleaned up, vram freed: 2.33 gb remaining\n",
      "2026-02-19 11:24:59,800 [INFO] loading base model into gpu (4-bit quantization)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 90bc2299-49d4-400f-85bc-f3c4aa7f52b0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:09,997 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 90bc2299-49d4-400f-85bc-f3c4aa7f52b0)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:09,999 [WARNING] Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bf7f9ebe-597a-4025-8934-8b519d287eef)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:21,073 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: bf7f9ebe-597a-4025-8934-8b519d287eef)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:21,075 [WARNING] Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2932b7ab-ef9d-4990-8315-9926642f4667)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:33,168 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2932b7ab-ef9d-4990-8315-9926642f4667)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:33,170 [WARNING] Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ac5cff80-dd97-47e5-bd41-a6a5a5ee703a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:47,273 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: ac5cff80-dd97-47e5-bd41-a6a5a5ee703a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:25:47,275 [WARNING] Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7ee49f0e-fd1b-43fa-a237-137a7ab4afa8)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:05,333 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 7ee49f0e-fd1b-43fa-a237-137a7ab4afa8)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:05,336 [WARNING] Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2ae97a89-6026-44a9-9f82-81c22f62d54c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:23,533 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2ae97a89-6026-44a9-9f82-81c22f62d54c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/config.json\n",
      "2026-02-19 11:26:23,753 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d3023800-d9e2-48ec-b255-9fadf84a1cd1)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:37,083 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: d3023800-d9e2-48ec-b255-9fadf84a1cd1)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:37,085 [WARNING] Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1e21ea3a-aae9-4c40-b026-3414a73465a2)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:48,159 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 1e21ea3a-aae9-4c40-b026-3414a73465a2)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:26:48,160 [WARNING] Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a15fb911-c43d-4b5b-aa90-fbe80ecc6871)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:00,248 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a15fb911-c43d-4b5b-aa90-fbe80ecc6871)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:00,250 [WARNING] Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2dda1133-05f0-4789-af6c-8e4cf57ee20c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:14,396 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 2dda1133-05f0-4789-af6c-8e4cf57ee20c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:14,397 [WARNING] Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 92d443e2-ff6b-4bb7-8d30-080653dc1806)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:32,500 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 92d443e2-ff6b-4bb7-8d30-080653dc1806)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:32,502 [WARNING] Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b24adc7e-c195-413e-bead-ad06aed87f9a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:27:50,596 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b24adc7e-c195-413e-bead-ad06aed87f9a)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/generation_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8e1ba225-70a9-4e18-b5c4-cf7b0574fe78)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:00,678 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 8e1ba225-70a9-4e18-b5c4-cf7b0574fe78)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:00,689 [WARNING] Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 883e0fdb-5400-4cd5-a2b2-e95f7ba59698)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:11,767 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 883e0fdb-5400-4cd5-a2b2-e95f7ba59698)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:11,769 [WARNING] Retrying in 2s [Retry 2/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b5f44b56-edf4-489e-ad19-79fc6d38e0a4)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:23,816 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: b5f44b56-edf4-489e-ad19-79fc6d38e0a4)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:23,818 [WARNING] Retrying in 4s [Retry 3/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9000fd46-057e-48e0-b9bf-c5cd6a14098c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:37,899 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 9000fd46-057e-48e0-b9bf-c5cd6a14098c)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:37,900 [WARNING] Retrying in 8s [Retry 4/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a63b5d6d-b4e8-4c05-b35b-3152d056da2b)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:55,981 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: a63b5d6d-b4e8-4c05-b35b-3152d056da2b)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:28:55,983 [WARNING] Retrying in 8s [Retry 5/5].\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88ab9935-7b93-45b0-9ca2-7f1e99fbe6c6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:29:14,067 [WARNING] '(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88ab9935-7b93-45b0-9ca2-7f1e99fbe6c6)')' thrown while requesting HEAD https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0/resolve/main/custom_generate/generate.py\n",
      "2026-02-19 11:29:14,124 [INFO] model loaded, vram: 3.05 gb\n",
      "2026-02-19 11:29:14,125 [INFO] generating base model responses on domain questions\n",
      "2026-02-19 11:29:41,407 [INFO] q: what is a transformer architecture in deep learning?\n",
      "2026-02-19 11:29:41,408 [INFO] a: A Transformer Architecture (Transformer) is a type of artificial intelligence model that uses self-attention mechanisms to process text data or natural language inputs. It can be used for various tasks such as language modeling, question answering, NLP, chatbots, and more. The Transformer architectu\n",
      "\n",
      "2026-02-19 11:30:26,441 [INFO] q: explain the difference between lstm and gru networks\n",
      "2026-02-19 11:30:26,443 [INFO] a: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are two types of recurrent neural network (RNN) models that differ in their architecture design. Here's a brief explanation:\n",
      "\n",
      "1. LSTM vs. RNN:\n",
      "\n",
      "An RNN is a type of neural network that uses backpropagation to learn patterns from input data.\n",
      "\n",
      "2026-02-19 11:31:13,363 [INFO] q: what is backpropagation and why is it important?\n",
      "2026-02-19 11:31:13,366 [INFO] a: Backpropagation (BP) is a type of artificial neural network algorithm used for training AI models with gradient descent optimization. It works by iteratively updating the weights of neurons based on their output and input gradients using the concept of the backward pass from each layer to the preced\n",
      "\n",
      "2026-02-19 11:31:58,610 [INFO] q: how does dropout regularization work?\n",
      "2026-02-19 11:31:58,611 [INFO] a: Dropout regularization is a technique used for training artificial neural networks (ANNs) to prevent overfitting or the tendency of the model to memorize the input data too much during training. The basic idea behind dropout is that by randomly dropping some neurons from each layer, the network lear\n",
      "\n",
      "2026-02-19 11:32:47,431 [INFO] q: what is transfer learning and when should you use it?\n",
      "2026-02-19 11:32:47,432 [INFO] a: Transfer Learning or Transferred Learning (TL) refers to the process of using pre-trained models from other tasks for fine-tuning on a new task with a different input format. This technique involves training a model on data that has been previously used to train another model that has achieved good \n",
      "\n",
      "2026-02-19 11:33:30,581 [INFO] q: explain the attention mechanism in neural networks\n",
      "2026-02-19 11:33:30,582 [INFO] a: The attention mechanism is a key component of many state-of-the-art neural networks used for natural language processing (NLP) tasks such as text classification, machine translation, and question answering. It helps to improve the model's ability to understand contextual information by attending to \n",
      "\n",
      "2026-02-19 11:34:13,520 [INFO] q: what is the vanishing gradient problem?\n",
      "2026-02-19 11:34:13,521 [INFO] a: The vanishing gradient problem refers to a phenomenon whereby the gradient of some function (usually a neural network's output) becomes zero at or near its minima, which can lead to poor performance on tasks that require smooth gradients like training convergence or optimization. The cause of this i\n",
      "\n",
      "2026-02-19 11:34:55,600 [INFO] q: how do generative adversarial networks work?\n",
      "2026-02-19 11:34:55,602 [INFO] a: Generative Adversarial Networks (GANs) is a type of artificial intelligence model that uses two contrasting models: the generator, which generates fake data or images, and the discriminator, which identifies whether the generated output is real or not. The process involves training both the generato\n",
      "\n",
      "2026-02-19 11:35:36,713 [INFO] q: what is the difference between supervised and unsupervised learning?\n",
      "2026-02-19 11:35:36,714 [INFO] a: Supervised learning involves providing labeled data to a model that learns from these data points. In this case, the model is trained on a set of examples with known labels for each example. Unsupervised learning, on the other hand, does not require any labeling information as it focuses more on und\n",
      "\n",
      "2026-02-19 11:36:19,000 [INFO] q: explain batch normalization and its benefits\n",
      "2026-02-19 11:36:19,002 [INFO] a: Batch Normalization (BN) is a technique used for training deep convolutional neural networks (CNNs). It can be considered as one of the most critical techniques to improve the performance of CNNs. Batch normalization adds another layer between the input data and the activation output layers. The add\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative Metrics: BLEU, ROUGE, and Token-Level F1"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:48:35.900887Z",
     "start_time": "2026-02-19T10:48:35.527489Z"
    }
   },
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def token_f1(prediction, reference):\n",
    "    pred_tokens = set(prediction.lower().split())\n",
    "    ref_tokens = set(reference.lower().split())\n",
    "    if not pred_tokens or not ref_tokens:\n",
    "        return 0.0\n",
    "    common = pred_tokens & ref_tokens\n",
    "    if not common:\n",
    "        return 0.0\n",
    "    p = len(common) / len(pred_tokens)\n",
    "    r = len(common) / len(ref_tokens)\n",
    "    return 2 * p * r / (p + r)\n",
    "\n",
    "def compute_all_metrics(predictions, references):\n",
    "    refs_for_bleu = [[r.split()] for r in references]\n",
    "    preds_for_bleu = [p.split() for p in predictions]\n",
    "    smooth = SmoothingFunction().method1\n",
    "    bleu = corpus_bleu(refs_for_bleu, preds_for_bleu, smoothing_function=smooth)\n",
    "\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "    rouge_scores = {\"rouge1\": [], \"rouge2\": [], \"rougeL\": []}\n",
    "    for pred, ref in zip(predictions, references):\n",
    "        scores = scorer.score(ref, pred)\n",
    "        for k in rouge_scores:\n",
    "            rouge_scores[k].append(scores[k].fmeasure)\n",
    "\n",
    "    f1_scores = [token_f1(p, r) for p, r in zip(predictions, references)]\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "\n",
    "    return {\n",
    "        \"bleu\": bleu,\n",
    "        \"rouge1\": np.mean(rouge_scores[\"rouge1\"]),\n",
    "        \"rouge2\": np.mean(rouge_scores[\"rouge2\"]),\n",
    "        \"rougeL\": np.mean(rouge_scores[\"rougeL\"]),\n",
    "        \"token_f1\": avg_f1,\n",
    "    }\n",
    "\n",
    "ft_metrics = compute_all_metrics(finetuned_responses, reference_answers)\n",
    "base_metrics = compute_all_metrics(base_responses, reference_answers)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    \"metric\": [\"bleu\", \"rouge-1\", \"rouge-2\", \"rouge-l\", \"token f1\"],\n",
    "    \"base_model\": [\n",
    "        f'{base_metrics[\"bleu\"]:.4f}',\n",
    "        f'{base_metrics[\"rouge1\"]:.4f}',\n",
    "        f'{base_metrics[\"rouge2\"]:.4f}',\n",
    "        f'{base_metrics[\"rougeL\"]:.4f}',\n",
    "        f'{base_metrics[\"token_f1\"]:.4f}',\n",
    "    ],\n",
    "    \"fine_tuned\": [\n",
    "        f'{ft_metrics[\"bleu\"]:.4f}',\n",
    "        f'{ft_metrics[\"rouge1\"]:.4f}',\n",
    "        f'{ft_metrics[\"rouge2\"]:.4f}',\n",
    "        f'{ft_metrics[\"rougeL\"]:.4f}',\n",
    "        f'{ft_metrics[\"token_f1\"]:.4f}',\n",
    "    ],\n",
    "})\n",
    "\n",
    "log.info(\"evaluation metrics comparison (base vs fine-tuned):\")\n",
    "table_str = metrics_df.to_string(index=False)\n",
    "for line in table_str.split(chr(10)):\n",
    "    log.info(line)\n",
    "\n",
    "for metric_name in [\"bleu\", \"rouge1\", \"rouge2\", \"rougeL\", \"token_f1\"]:\n",
    "    base_val = base_metrics[metric_name]\n",
    "    ft_val = ft_metrics[metric_name]\n",
    "    if base_val > 0:\n",
    "        improvement = ((ft_val - base_val) / base_val) * 100\n",
    "        log.info(f\"{metric_name} improvement: {improvement:+.1f}%\")\n",
    "\n",
    "metrics_df.to_csv(f\"{LOG_DIR}/metrics.csv\", index=False)\n",
    "log.info(f\"metrics saved to {LOG_DIR}/metrics.csv\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:48:35,640 [INFO] Using default tokenizer.\n",
      "2026-02-19 11:48:35,719 [INFO] Using default tokenizer.\n",
      "2026-02-19 11:48:35,846 [INFO] evaluation metrics comparison (base vs fine-tuned):\n",
      "2026-02-19 11:48:35,855 [INFO]   metric base_model fine_tuned\n",
      "2026-02-19 11:48:35,857 [INFO]     bleu     0.0163     0.0065\n",
      "2026-02-19 11:48:35,858 [INFO]  rouge-1     0.2717     0.3302\n",
      "2026-02-19 11:48:35,860 [INFO]  rouge-2     0.0695     0.0624\n",
      "2026-02-19 11:48:35,864 [INFO]  rouge-l     0.1632     0.1960\n",
      "2026-02-19 11:48:35,867 [INFO] token f1     0.2453     0.2775\n",
      "2026-02-19 11:48:35,869 [INFO] bleu improvement: -59.9%\n",
      "2026-02-19 11:48:35,872 [INFO] rouge1 improvement: +21.5%\n",
      "2026-02-19 11:48:35,875 [INFO] rouge2 improvement: -10.1%\n",
      "2026-02-19 11:48:35,877 [INFO] rougeL improvement: +20.1%\n",
      "2026-02-19 11:48:35,885 [INFO] token_f1 improvement: +13.1%\n",
      "2026-02-19 11:48:35,894 [INFO] metrics saved to ./logs/metrics.csv\n"
     ]
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perplexity Comparison"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:54:18.377188Z",
     "start_time": "2026-02-19T10:50:50.844094Z"
    }
   },
   "source": "def compute_perplexity(mdl, tok, texts, max_length=512):\n    total_loss = 0\n    total_tokens = 0\n    for text in texts:\n        inputs = tok(text, return_tensors=\"pt\", truncation=True, max_length=max_length).to(mdl.device)\n        with torch.no_grad():\n            outputs = mdl(**inputs, labels=inputs[\"input_ids\"])\n        total_loss += outputs.loss.item() * inputs[\"input_ids\"].shape[1]\n        total_tokens += inputs[\"input_ids\"].shape[1]\n    return np.exp(total_loss / total_tokens)\n\neval_texts = [ex[\"text\"] for ex in eval_data.select(range(min(50, len(eval_data))))]\n\nbase_perplexity = compute_perplexity(base_model, tokenizer, eval_texts)\nlog.info(f\"base model perplexity: {base_perplexity:.2f}\")\n\ncleanup_model(base_model)\n\nft_model = load_base_model()\nft_model = PeftModel.from_pretrained(ft_model, BEST_MODEL_PATH)\n\nft_perplexity = compute_perplexity(ft_model, tokenizer, eval_texts)\nlog.info(f\"fine-tuned model perplexity: {ft_perplexity:.2f}\")\nlog.info(f\"perplexity improvement: {((base_perplexity - ft_perplexity) / base_perplexity * 100):.1f}%\")\n\nwith open(f\"{LOG_DIR}/perplexity.json\", \"w\") as f:\n    json.dump({\"base\": round(base_perplexity, 2), \"fine_tuned\": round(ft_perplexity, 2)}, f, indent=2)\nlog.info(f\"perplexity results saved to {LOG_DIR}/perplexity.json\")",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-19 11:52:29,394 [INFO] base model perplexity: 7.82\n",
      "2026-02-19 11:52:30,038 [INFO] model cleaned up, vram freed: 2.36 gb remaining\n",
      "2026-02-19 11:52:30,042 [INFO] loading base model into gpu (4-bit quantization)...\n",
      "2026-02-19 11:52:31,457 [INFO] We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n",
      "2026-02-19 11:52:37,499 [INFO] model loaded, vram: 3.08 gb\n",
      "2026-02-19 11:54:18,365 [INFO] fine-tuned model perplexity: 3.54\n",
      "2026-02-19 11:54:18,366 [INFO] perplexity improvement: 54.7%\n",
      "2026-02-19 11:54:18,373 [INFO] perplexity results saved to ./logs/perplexity.json\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qualitative Comparison: Base vs Fine-Tuned\n",
    "\n",
    "Side-by-side responses demonstrate how fine-tuning improves domain-specific answer quality, relevance, and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-02-19T10:56:52.001527Z",
     "start_time": "2026-02-19T10:56:51.996387Z"
    }
   },
   "source": [
    "print(\"side-by-side comparison (base vs fine-tuned):\\n\")\n",
    "for i, q in enumerate(test_questions[:5]):\n",
    "    print(f\"question: {q}\")\n",
    "    print(f\"  base model: {base_responses[i][:300]}\")\n",
    "    print(f\"  fine-tuned: {finetuned_responses[i][:300]}\")\n",
    "    print()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "side-by-side comparison (base vs fine-tuned):\n",
      "\n",
      "question: what is a transformer architecture in deep learning?\n",
      "  base model: A Transformer Architecture (Transformer) is a type of artificial intelligence model that uses self-attention mechanisms to process text data or natural language inputs. It can be used for various tasks such as language modeling, question answering, NLP, chatbots, and more. The Transformer architectu\n",
      "  fine-tuned: A Transformer Architecture (Transformer) is a state-of-the-art natural language processing model that uses stacked encoderdecoders to process text data. It consists of multiple layers with attention mechanisms to capture contextual information from the input sequence while also learning representat\n",
      "\n",
      "question: explain the difference between lstm and gru networks\n",
      "  base model: LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) are two types of recurrent neural network (RNN) models that differ in their architecture design. Here's a brief explanation:\n",
      "\n",
      "1. LSTM vs. RNN:\n",
      "\n",
      "An RNN is a type of neural network that uses backpropagation to learn patterns from input data.\n",
      "  fine-tuned: LSTM (Long Short-Term Memory) is a type of recurrent neural network that can process sequential data such as text or speech. It uses a hidden state to remember previous input sequences and output them with a probability distribution over time. This allows it to create complex hierarchical structures\n",
      "\n",
      "question: what is backpropagation and why is it important?\n",
      "  base model: Backpropagation (BP) is a type of artificial neural network algorithm used for training AI models with gradient descent optimization. It works by iteratively updating the weights of neurons based on their output and input gradients using the concept of the backward pass from each layer to the preced\n",
      "  fine-tuned: Backpropagation (BP) is a type of artificial intelligence algorithm that trains a neural network to accurately identify patterns within data. It uses error signals generated by the training process as feedback, which helps the network adjust its weights over time. By doing so, BP can learn from past\n",
      "\n",
      "question: how does dropout regularization work?\n",
      "  base model: Dropout regularization is a technique used for training artificial neural networks (ANNs) to prevent overfitting or the tendency of the model to memorize the input data too much during training. The basic idea behind dropout is that by randomly dropping some neurons from each layer, the network lear\n",
      "  fine-tuned: Dropout is a technique that can be used to regularize the output of a deep neural network by randomly removing neurons from its input layers. This allows for the model to learn without overfitting or getting stuck in local minima. By adding random noise to each input sample, Dropout prevents the mod\n",
      "\n",
      "question: what is transfer learning and when should you use it?\n",
      "  base model: Transfer Learning or Transferred Learning (TL) refers to the process of using pre-trained models from other tasks for fine-tuning on a new task with a different input format. This technique involves training a model on data that has been previously used to train another model that has achieved good \n",
      "  fine-tuned: Transfer Learning is a technique that involves taking pre-trained models from other tasks to train on new data. The benefit of using Transfer Learning is that the model can be trained with little or no additional training time compared to traditional methods for similar problems. However, this metho\n",
      "\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Analysis\n",
    "\n",
    "The evaluation above compares the base pre-trained TinyLlama model against our fine-tuned version across multiple dimensions:\n",
    "\n",
    "**BLEU and ROUGE** measure how closely the generated responses match reference answers in terms of n-gram overlap. Higher scores for the fine-tuned model indicate it produces responses that are more aligned with expected ML/AI explanations.\n",
    "\n",
    "**Token-level F1** captures both precision (are the generated tokens relevant?) and recall (are important reference tokens present?). Improvement here shows the fine-tuned model better covers key terminology and concepts.\n",
    "\n",
    "**Perplexity** measures how confident the model is on domain-specific text. Lower perplexity after fine-tuning indicates the model has internalized ML/AI language patterns and can predict domain text more accurately.\n",
    "\n",
    "**Out-of-domain testing** verifies that the model handles non-ML queries gracefully rather than hallucinating ML-related content for unrelated questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Web Interface (React + FastAPI)\n\nThe fine-tuned model is deployed via a FastAPI backend (`backend/app.py`) that serves a `/chat` endpoint, `/metrics` for evaluation results, and `/model-info` for architecture details. The React frontend (`frontend/`) provides a feature-rich chat UI with dark/light mode, model info panel, metrics display, typing animation, and example questions.\n\nTo run:\n1. Start backend: `python -m uvicorn backend.app:app --reload`\n2. Start frontend: `cd frontend && npm start`"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LoRA Venv",
   "language": "python",
   "name": "lora-venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
